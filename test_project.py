#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®åŠŸèƒ½å®Œæ•´æµ‹è¯•ä»£ç 
æµ‹è¯•æ‰€æœ‰æ¨¡å—çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
"""

import os
import sys
import time
import tempfile
import shutil
import unittest
import pandas as pd
import numpy as np
from unittest.mock import patch, MagicMock
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import Config, ModelConfig, FAISSConfig, ProcessingConfig, DataConfig, LogConfig
from logger import Logger, setup_logger
from data_processor import TextCleaner, DataLoader, BusinessScopeProcessor, DataProcessor
from matcher import ModelManager, FAISSIndexManager, OptimizedMatcher
from main import JobMatcher

class TestConfig(unittest.TestCase):
    """é…ç½®æ¨¡å—æµ‹è¯•"""
    
    def test_model_config(self):
        """æµ‹è¯•æ¨¡å‹é…ç½®"""
        config = ModelConfig()
        self.assertEqual(config.name, 'BAAI/bge-small-zh-v1.5')
        self.assertEqual(config.batch_size, 64)
        self.assertEqual(config.device, 'auto')
    
    def test_faiss_config(self):
        """æµ‹è¯•FAISSé…ç½®"""
        config = FAISSConfig()
        self.assertEqual(config.index_type, 'IndexFlatIP')
        self.assertTrue(config.normalize_l2)
    
    def test_processing_config(self):
        """æµ‹è¯•å¤„ç†é…ç½®"""
        config = ProcessingConfig()
        self.assertEqual(config.batch_size, 1000)
        self.assertIsNotNone(config.drop_texts)
        self.assertIn('åœ¨ä¸­å›½æ³•å¾‹å…è®¸', config.drop_texts)
    
    def test_config_from_dict(self):
        """æµ‹è¯•ä»å­—å…¸åˆ›å»ºé…ç½®"""
        config_dict = {
            'model': {'batch_size': 128, 'device': 'cpu'},
            'processing': {'batch_size': 500}
        }
        config = Config.from_dict(config_dict)
        self.assertEqual(config.model.batch_size, 128)
        self.assertEqual(config.model.device, 'cpu')
        self.assertEqual(config.processing.batch_size, 500)

class TestLogger(unittest.TestCase):
    """æ—¥å¿—æ¨¡å—æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        self.log_file = os.path.join(self.temp_dir, 'test.log')
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.temp_dir)
    
    def test_logger_setup(self):
        """æµ‹è¯•æ—¥å¿—è®¾ç½®"""
        log_config = LogConfig(level='DEBUG', file=self.log_file)
        logger = Logger()
        logger.setup(log_config)
        
        # æµ‹è¯•æ—¥å¿—è®°å½•
        logger.info("æµ‹è¯•ä¿¡æ¯")
        logger.warning("æµ‹è¯•è­¦å‘Š")
        logger.error("æµ‹è¯•é”™è¯¯")
        
        # éªŒè¯æ—¥å¿—æ–‡ä»¶å­˜åœ¨
        self.assertTrue(os.path.exists(self.log_file))
        
        # éªŒè¯æ—¥å¿—å†…å®¹
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("æµ‹è¯•ä¿¡æ¯", content)
            self.assertIn("æµ‹è¯•è­¦å‘Š", content)
            self.assertIn("æµ‹è¯•é”™è¯¯", content)

class TestTextCleaner(unittest.TestCase):
    """æ–‡æœ¬æ¸…ç†å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        config = ProcessingConfig(cache_dir=self.temp_dir)
        self.cleaner = TextCleaner(config)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.temp_dir)
    
    def test_clean_text_basic(self):
        """æµ‹è¯•åŸºæœ¬æ–‡æœ¬æ¸…ç†"""
        # æµ‹è¯•HTMLæ ‡ç­¾åˆ é™¤
        text = "<p>è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•</p>"
        cleaned = self.cleaner.clean_text(text)
        self.assertNotIn('<p>', cleaned)
        self.assertNotIn('</p>', cleaned)
        
        # æµ‹è¯•URLåˆ é™¤
        text = "è®¿é—® https://www.example.com è·å–æ›´å¤šä¿¡æ¯"
        cleaned = self.cleaner.clean_text(text)
        self.assertNotIn('https://www.example.com', cleaned)
        
        # æµ‹è¯•æ•°å­—åˆ é™¤
        text = "å…¬å¸æˆç«‹äº2020å¹´"
        cleaned = self.cleaner.clean_text(text)
        self.assertNotIn('2020', cleaned)
        
        # æµ‹è¯•æ‹¬å·åˆ é™¤
        text = "è½¯ä»¶å¼€å‘ï¼ˆåŒ…æ‹¬ç§»åŠ¨åº”ç”¨ï¼‰"
        cleaned = self.cleaner.clean_text(text)
        self.assertNotIn('ï¼ˆ', cleaned)
        self.assertNotIn('ï¼‰', cleaned)
    
    def test_vectorized_clean(self):
        """æµ‹è¯•æ‰¹é‡æ–‡æœ¬æ¸…ç†"""
        texts = [
            "<p>è½¯ä»¶å¼€å‘</p>",
            "ç¡¬ä»¶é”€å”®ï¼ˆé›¶å”®ï¼‰",
            "åœ¨ä¸­å›½æ³•å¾‹å…è®¸çš„èŒƒå›´å†…",
            "å’¨è¯¢æœåŠ¡123"
        ]
        
        cleaned = self.cleaner.vectorized_clean(texts)
        
        # éªŒè¯ç»“æœæ•°é‡
        self.assertEqual(len(cleaned), len(texts))
        
        # éªŒè¯æ¸…ç†æ•ˆæœ
        self.assertNotIn('<p>', cleaned[0])
        self.assertNotIn('ï¼ˆ', cleaned[1])
        self.assertEqual(cleaned[2], '')  # åº”è¯¥è¢«è¿‡æ»¤æ‰
        self.assertNotIn('123', cleaned[3])
    
    def test_empty_and_invalid_input(self):
        """æµ‹è¯•ç©ºå€¼å’Œæ— æ•ˆè¾“å…¥"""
        # æµ‹è¯•ç©ºå­—ç¬¦ä¸²
        self.assertEqual(self.cleaner.clean_text(''), '')
        self.assertEqual(self.cleaner.clean_text('   '), '')
        
        # æµ‹è¯•Noneå€¼
        self.assertEqual(self.cleaner.clean_text(None), '')
        
        # æµ‹è¯•éå­—ç¬¦ä¸²ç±»å‹
        self.assertEqual(self.cleaner.clean_text(123), '')

class TestDataLoader(unittest.TestCase):
    """æ•°æ®åŠ è½½å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºæµ‹è¯•è¡Œä¸šæ•°æ®
        self.industry_file = os.path.join(self.temp_dir, 'industry.csv')
        industry_data = pd.DataFrame({
            'è¡Œä¸šä»£ç ': ['A01', 'A02', 'B01'],
            'è¡Œä¸šåç§°': ['å†œä¸š', 'æ—ä¸š', 'åˆ¶é€ ä¸š']
        })
        industry_data.to_csv(self.industry_file, index=False, encoding='utf-8')
        
        # åˆ›å»ºæµ‹è¯•ä¼ä¸šæ•°æ®ç›®å½•
        self.chunks_dir = os.path.join(self.temp_dir, 'chunks')
        os.makedirs(self.chunks_dir)
        
        # åˆ›å»ºæµ‹è¯•ä¼ä¸šæ•°æ®æ–‡ä»¶
        chunk1_file = os.path.join(self.chunks_dir, 'chunk1.csv')
        chunk1_data = pd.DataFrame({
            'ä¼ä¸šåç§°': ['å…¬å¸A', 'å…¬å¸B'],
            'ç»è¥èŒƒå›´': ['è½¯ä»¶å¼€å‘', 'ç¡¬ä»¶é”€å”®']
        })
        chunk1_data.to_csv(chunk1_file, index=False, encoding='utf-8')
        
        chunk2_file = os.path.join(self.chunks_dir, 'chunk2.csv')
        chunk2_data = pd.DataFrame({
            'ä¼ä¸šåç§°': ['å…¬å¸C'],
            'ç»è¥èŒƒå›´': ['å’¨è¯¢æœåŠ¡']
        })
        chunk2_data.to_csv(chunk2_file, index=False, encoding='utf-8')
        
        # åˆ›å»ºé…ç½®
        self.config = DataConfig(
            industry_file=self.industry_file,
            chunks_dir=self.chunks_dir,
            output_file=os.path.join(self.temp_dir, 'output.csv')
        )
        self.loader = DataLoader(self.config)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.temp_dir)
    
    def test_load_industry_data(self):
        """æµ‹è¯•åŠ è½½è¡Œä¸šæ•°æ®"""
        df = self.loader.load_industry_data()
        
        # éªŒè¯æ•°æ®ç»“æ„
        self.assertIn('è¡Œä¸šä»£ç ', df.columns)
        self.assertIn('è¡Œä¸šåç§°', df.columns)
        self.assertEqual(len(df), 3)
        
        # éªŒè¯æ•°æ®å†…å®¹
        self.assertIn('A01', df['è¡Œä¸šä»£ç '].values)
        self.assertIn('å†œä¸š', df['è¡Œä¸šåç§°'].values)
    
    def test_load_enterprise_chunks(self):
        """æµ‹è¯•åŠ è½½ä¼ä¸šæ•°æ®åˆ†å—"""
        chunks = self.loader.load_enterprise_chunks()
        
        # éªŒè¯åˆ†å—æ•°é‡
        self.assertEqual(len(chunks), 2)
        
        # éªŒè¯åˆ†å—å†…å®¹
        chunk_names = [chunk[0] for chunk in chunks]
        self.assertIn('chunk1.csv', chunk_names)
        self.assertIn('chunk2.csv', chunk_names)
        
        # éªŒè¯æ•°æ®å†…å®¹
        total_records = sum(len(chunk[1]) for chunk in chunks)
        self.assertEqual(total_records, 3)
    
    def test_save_results(self):
        """æµ‹è¯•ä¿å­˜ç»“æœ"""
        results = [
            {'ä¼ä¸šåç§°': 'å…¬å¸A', 'è¡Œä¸šä»£ç ': 'A01', 'ç›¸ä¼¼åº¦': 0.95},
            {'ä¼ä¸šåç§°': 'å…¬å¸B', 'è¡Œä¸šä»£ç ': 'B01', 'ç›¸ä¼¼åº¦': 0.88}
        ]
        
        output_file = self.loader.save_results(results)
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        self.assertTrue(os.path.exists(output_file))
        
        # éªŒè¯æ–‡ä»¶å†…å®¹
        df = pd.read_csv(output_file, encoding='utf-8')
        self.assertEqual(len(df), 2)
        self.assertIn('ä¼ä¸šåç§°', df.columns)
        self.assertIn('è¡Œä¸šä»£ç ', df.columns)
        self.assertIn('ç›¸ä¼¼åº¦', df.columns)

class TestBusinessScopeProcessor(unittest.TestCase):
    """ç»è¥èŒƒå›´å¤„ç†å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        config = ProcessingConfig()
        text_cleaner = TextCleaner(config)
        self.processor = BusinessScopeProcessor(text_cleaner)
    
    def test_process_business_scope(self):
        """æµ‹è¯•å¤„ç†å•ä¸ªç»è¥èŒƒå›´"""
        scope_text = "è½¯ä»¶å¼€å‘ï¼›ç¡¬ä»¶é”€å”®ï¼ŒæŠ€æœ¯å’¨è¯¢ã€‚"
        segments = self.processor.process_business_scope(scope_text)
        
        # éªŒè¯åˆ†å‰²æ•ˆæœ
        self.assertGreater(len(segments), 1)
        
        # éªŒè¯åŒ…å«é¢„æœŸå†…å®¹
        segment_text = ' '.join(segments)
        self.assertIn('è½¯ä»¶å¼€å‘', segment_text)
        self.assertIn('ç¡¬ä»¶é”€å”®', segment_text)
        self.assertIn('æŠ€æœ¯å’¨è¯¢', segment_text)
    
    def test_process_batch(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†ç»è¥èŒƒå›´"""
        scope_texts = [
            "è½¯ä»¶å¼€å‘ï¼›ç¡¬ä»¶é”€å”®",
            "æŠ€æœ¯å’¨è¯¢ï¼Œç®¡ç†å’¨è¯¢",
            ""  # ç©ºå­—ç¬¦ä¸²
        ]
        
        results = self.processor.process_batch(scope_texts)
        
        # éªŒè¯ç»“æœæ•°é‡
        self.assertEqual(len(results), 3)
        
        # éªŒè¯éç©ºç»“æœ
        self.assertGreater(len(results[0]), 0)
        self.assertGreater(len(results[1]), 0)
        self.assertEqual(len(results[2]), 0)  # ç©ºå­—ç¬¦ä¸²åº”è¯¥è¿”å›ç©ºåˆ—è¡¨

class TestDataProcessor(unittest.TestCase):
    """æ•°æ®å¤„ç†å™¨ä¸»ç±»æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        industry_file = os.path.join(self.temp_dir, 'industry.csv')
        industry_data = pd.DataFrame({
            'è¡Œä¸šä»£ç ': ['A01', 'A02'],
            'è¡Œä¸šåç§°': ['è½¯ä»¶å¼€å‘', 'ç¡¬ä»¶åˆ¶é€ ']
        })
        industry_data.to_csv(industry_file, index=False, encoding='utf-8')
        
        chunks_dir = os.path.join(self.temp_dir, 'chunks')
        os.makedirs(chunks_dir)
        
        # åˆ›å»ºé…ç½®
        processing_config = ProcessingConfig()
        data_config = DataConfig(
            industry_file=industry_file,
            chunks_dir=chunks_dir
        )
        
        self.processor = DataProcessor(processing_config, data_config)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.temp_dir)
    
    def test_load_and_process_industry_data(self):
        """æµ‹è¯•åŠ è½½å¹¶å¤„ç†è¡Œä¸šæ•°æ®"""
        industry_df, industry_names = self.processor.load_and_process_industry_data()
        
        # éªŒè¯è¿”å›ç»“æœ
        self.assertIsInstance(industry_df, pd.DataFrame)
        self.assertIsInstance(industry_names, list)
        
        # éªŒè¯æ•°æ®å†…å®¹
        self.assertGreater(len(industry_df), 0)
        self.assertGreater(len(industry_names), 0)
        self.assertEqual(len(industry_df), len(industry_names))

class TestModelManager(unittest.TestCase):
    """æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
        config = ModelConfig(name='sentence-transformers/all-MiniLM-L6-v2', batch_size=2)
        self.manager = ModelManager(config)
    
    @patch('torch.cuda.is_available')
    def test_get_device(self, mock_cuda):
        """æµ‹è¯•è®¾å¤‡æ£€æµ‹"""
        # æµ‹è¯•CUDAå¯ç”¨
        mock_cuda.return_value = True
        device = self.manager._get_device()
        self.assertEqual(device, 'cuda')
        
        # æµ‹è¯•CUDAä¸å¯ç”¨
        mock_cuda.return_value = False
        device = self.manager._get_device()
        self.assertEqual(device, 'cpu')
    
    def test_encode_texts(self):
        """æµ‹è¯•æ–‡æœ¬ç¼–ç ï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼‰"""
        # è·³è¿‡éœ€è¦ä¸‹è½½æ¨¡å‹çš„æµ‹è¯•
        self.skipTest("è·³è¿‡éœ€è¦ä¸‹è½½æ¨¡å‹çš„æµ‹è¯•")
        
        texts = ['è½¯ä»¶å¼€å‘', 'ç¡¬ä»¶é”€å”®']
        embeddings = self.manager.encode_texts(texts)
        
        # éªŒè¯ç¼–ç ç»“æœ
        self.assertEqual(len(embeddings), 2)
        self.assertGreater(embeddings.shape[1], 0)  # éªŒè¯å‘é‡ç»´åº¦

class TestFAISSIndexManager(unittest.TestCase):
    """FAISSç´¢å¼•ç®¡ç†å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        config = FAISSConfig()
        self.manager = FAISSIndexManager(config)
    
    def test_build_index(self):
        """æµ‹è¯•æ„å»ºç´¢å¼•"""
        # åˆ›å»ºæµ‹è¯•å‘é‡
        embeddings = np.random.rand(10, 128).astype(np.float32)
        
        # æ„å»ºç´¢å¼•
        index = self.manager.build_index(embeddings)
        
        # éªŒè¯ç´¢å¼•
        self.assertIsNotNone(index)
        self.assertEqual(index.ntotal, 10)
        self.assertEqual(self.manager.dimension, 128)
    
    def test_search(self):
        """æµ‹è¯•æœç´¢"""
        # åˆ›å»ºæµ‹è¯•å‘é‡
        embeddings = np.random.rand(10, 128).astype(np.float32)
        query_embeddings = np.random.rand(2, 128).astype(np.float32)
        
        # æ„å»ºç´¢å¼•
        self.manager.build_index(embeddings)
        
        # æ‰§è¡Œæœç´¢
        scores, indices = self.manager.search(query_embeddings, k=3)
        
        # éªŒè¯æœç´¢ç»“æœ
        self.assertEqual(scores.shape, (2, 3))
        self.assertEqual(indices.shape, (2, 3))
        self.assertTrue(np.all(indices >= 0))
        self.assertTrue(np.all(indices < 10))

class TestOptimizedMatcher(unittest.TestCase):
    """ä¼˜åŒ–åŒ¹é…å™¨æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        model_config = ModelConfig(name='sentence-transformers/all-MiniLM-L6-v2')
        faiss_config = FAISSConfig()
        self.matcher = OptimizedMatcher(model_config, faiss_config)
    
    def test_cache_functionality(self):
        """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
        # æµ‹è¯•ç¼“å­˜ä¿¡æ¯
        cache_info = self.matcher.get_cache_info()
        self.assertIn('cache_size', cache_info)
        self.assertIn('model_loaded', cache_info)
        self.assertIn('index_built', cache_info)
        
        # æµ‹è¯•æ¸…ç©ºç¼“å­˜
        self.matcher.clear_cache()
        cache_info = self.matcher.get_cache_info()
        self.assertEqual(cache_info['cache_size'], 0)

class TestJobMatcher(unittest.TestCase):
    """å·¥ä½œåŒ¹é…å™¨ä¸»ç±»æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = Config()
        config.log.file = os.path.join(self.temp_dir, 'test.log')
        config.log.console = False
        
        # æ¨¡æ‹Ÿæ•°æ®æ–‡ä»¶ï¼ˆä¸å®é™…åˆ›å»ºï¼Œç”¨äºæµ‹è¯•é…ç½®ï¼‰
        config.data.industry_file = os.path.join(self.temp_dir, 'industry.csv')
        config.data.chunks_dir = os.path.join(self.temp_dir, 'chunks')
        config.data.output_file = os.path.join(self.temp_dir, 'output.csv')
        
        self.matcher = JobMatcher(config)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.temp_dir)
    
    def test_validate_environment(self):
        """æµ‹è¯•ç¯å¢ƒéªŒè¯"""
        # ç”±äºç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼ŒéªŒè¯åº”è¯¥å¤±è´¥
        with self.assertRaises(FileNotFoundError):
            self.matcher.validate_environment()
    
    def test_print_statistics(self):
        """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯æ‰“å°"""
        # è®¾ç½®ä¸€äº›æµ‹è¯•ç»Ÿè®¡æ•°æ®
        self.matcher.stats = {
            'total_enterprises': 100,
            'processed_enterprises': 95,
            'matched_enterprises': 85,
            'total_time': 120.5,
            'load_time': 10.2,
            'process_time': 110.3
        }
        
        # æµ‹è¯•æ‰“å°ï¼ˆä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        try:
            self.matcher.print_statistics()
        except Exception as e:
            self.fail(f"print_statistics raised {e} unexpectedly!")

class TestIntegration(unittest.TestCase):
    """é›†æˆæµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®é›†æˆæµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºå®Œæ•´çš„æµ‹è¯•æ•°æ®
        self.setup_test_data()
        
        # åˆ›å»ºé…ç½®
        self.config = Config()
        self.config.data.industry_file = self.industry_file
        self.config.data.chunks_dir = self.chunks_dir
        self.config.data.output_file = os.path.join(self.temp_dir, 'results.csv')
        self.config.log.file = os.path.join(self.temp_dir, 'test.log')
        self.config.log.console = False
        
        # ä½¿ç”¨CPUå’Œå°æ‰¹æ¬¡è¿›è¡Œæµ‹è¯•
        self.config.model.device = 'cpu'
        self.config.model.batch_size = 2
    
    def tearDown(self):
        """æ¸…ç†é›†æˆæµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.temp_dir)
    
    def setup_test_data(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        # åˆ›å»ºè¡Œä¸šæ•°æ®
        self.industry_file = os.path.join(self.temp_dir, 'industry.csv')
        industry_data = pd.DataFrame({
            'è¡Œä¸šä»£ç ': ['C39', 'I65', 'M73'],
            'è¡Œä¸šåç§°': ['è®¡ç®—æœºã€é€šä¿¡å’Œå…¶ä»–ç”µå­è®¾å¤‡åˆ¶é€ ä¸š', 'è½¯ä»¶å’Œä¿¡æ¯æŠ€æœ¯æœåŠ¡ä¸š', 'ç§‘æŠ€æ¨å¹¿å’Œåº”ç”¨æœåŠ¡ä¸š']
        })
        industry_data.to_csv(self.industry_file, index=False, encoding='utf-8')
        
        # åˆ›å»ºä¼ä¸šæ•°æ®ç›®å½•å’Œæ–‡ä»¶
        self.chunks_dir = os.path.join(self.temp_dir, 'chunks')
        os.makedirs(self.chunks_dir)
        
        chunk_file = os.path.join(self.chunks_dir, 'test_chunk.csv')
        enterprise_data = pd.DataFrame({
            'ä¼ä¸šåç§°': ['ç§‘æŠ€å…¬å¸A', 'è½¯ä»¶å…¬å¸B', 'åˆ¶é€ å…¬å¸C'],
            'ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ': ['123456789', '987654321', '456789123'],
            'ç»è¥èŒƒå›´': [
                'è½¯ä»¶å¼€å‘ï¼›æŠ€æœ¯å’¨è¯¢ï¼›è®¡ç®—æœºç³»ç»ŸæœåŠ¡',
                'åº”ç”¨è½¯ä»¶æœåŠ¡ï¼›ä¿¡æ¯ç³»ç»Ÿé›†æˆæœåŠ¡',
                'ç”µå­äº§å“åˆ¶é€ ï¼›ç¡¬ä»¶è®¾å¤‡é”€å”®'
            ]
        })
        enterprise_data.to_csv(chunk_file, index=False, encoding='utf-8')
    
    def test_data_processing_pipeline(self):
        """æµ‹è¯•æ•°æ®å¤„ç†æµæ°´çº¿"""
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        processor = DataProcessor(self.config.processing, self.config.data)
        
        # æµ‹è¯•åŠ è½½å’Œå¤„ç†è¡Œä¸šæ•°æ®
        industry_df, industry_names = processor.load_and_process_industry_data()
        self.assertGreater(len(industry_df), 0)
        self.assertGreater(len(industry_names), 0)
        
        # æµ‹è¯•åŠ è½½ä¼ä¸šæ•°æ®
        enterprise_chunks = processor.load_enterprise_data()
        self.assertGreater(len(enterprise_chunks), 0)
        
        # æµ‹è¯•å¤„ç†ç»è¥èŒƒå›´
        chunk_name, chunk_df = enterprise_chunks[0]
        business_scopes = chunk_df['ç»è¥èŒƒå›´'].tolist()
        processed_scopes = processor.scope_processor.process_batch(business_scopes)
        
        self.assertEqual(len(processed_scopes), len(business_scopes))
        self.assertGreater(len(processed_scopes[0]), 0)  # ç¬¬ä¸€ä¸ªåº”è¯¥æœ‰å¤„ç†ç»“æœ
    
    def test_text_cleaning_pipeline(self):
        """æµ‹è¯•æ–‡æœ¬æ¸…ç†æµæ°´çº¿"""
        cleaner = TextCleaner(self.config.processing)
        
        # æµ‹è¯•å„ç§æ–‡æœ¬æ¸…ç†åœºæ™¯
        test_cases = [
            ("è½¯ä»¶å¼€å‘ï¼ˆåŒ…æ‹¬ç§»åŠ¨åº”ç”¨ï¼‰", "è½¯ä»¶å¼€å‘"),
            ("<p>ç½‘ç«™å»ºè®¾</p>", "ç½‘ç«™å»ºè®¾"),
            ("æŠ€æœ¯å’¨è¯¢123æœåŠ¡", "æŠ€æœ¯å’¨è¯¢æœåŠ¡"),
            ("åœ¨ä¸­å›½æ³•å¾‹å…è®¸çš„èŒƒå›´å†…ç»è¥", ""),  # åº”è¯¥è¢«è¿‡æ»¤
        ]
        
        for original, expected_contains in test_cases:
            cleaned = cleaner.clean_text(original)
            if expected_contains:
                self.assertIn(expected_contains, cleaned)
            else:
                self.assertEqual(cleaned, expected_contains)

def create_test_suite():
    """åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
    suite = unittest.TestSuite()
    
    # æ·»åŠ æ‰€æœ‰æµ‹è¯•ç±»
    test_classes = [
        TestConfig,
        TestLogger,
        TestTextCleaner,
        TestDataLoader,
        TestBusinessScopeProcessor,
        TestDataProcessor,
        TestFAISSIndexManager,
        TestOptimizedMatcher,
        TestJobMatcher,
        TestIntegration
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        suite.addTests(tests)
    
    return suite

def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "="*60)
    print("æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç¯å¢ƒ
    temp_dir = tempfile.mkdtemp()
    
    try:
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
        
        # è¡Œä¸šæ•°æ®
        industry_file = os.path.join(temp_dir, 'industry.csv')
        industry_data = pd.DataFrame({
            'è¡Œä¸šä»£ç ': [f'A{i:02d}' for i in range(100)],
            'è¡Œä¸šåç§°': [f'è¡Œä¸š{i}' for i in range(100)]
        })
        industry_data.to_csv(industry_file, index=False, encoding='utf-8')
        
        # ä¼ä¸šæ•°æ®
        chunks_dir = os.path.join(temp_dir, 'chunks')
        os.makedirs(chunks_dir)
        
        chunk_file = os.path.join(chunks_dir, 'perf_test.csv')
        enterprise_data = pd.DataFrame({
            'ä¼ä¸šåç§°': [f'å…¬å¸{i}' for i in range(1000)],
            'ç»è¥èŒƒå›´': [f'ä¸šåŠ¡{i}ï¼›æœåŠ¡{i}ï¼Œäº§å“{i}' for i in range(1000)]
        })
        enterprise_data.to_csv(chunk_file, index=False, encoding='utf-8')
        
        # æ€§èƒ½æµ‹è¯•é…ç½®
        config = ProcessingConfig(cache_dir=temp_dir)
        data_config = DataConfig(
            industry_file=industry_file,
            chunks_dir=chunks_dir
        )
        
        # æµ‹è¯•æ–‡æœ¬æ¸…ç†æ€§èƒ½
        print("æµ‹è¯•æ–‡æœ¬æ¸…ç†æ€§èƒ½...")
        cleaner = TextCleaner(config)
        
        test_texts = [f'è½¯ä»¶å¼€å‘{i}ï¼›ç¡¬ä»¶é”€å”®{i}ï¼ŒæŠ€æœ¯å’¨è¯¢{i}' for i in range(1000)]
        
        start_time = time.time()
        cleaned_texts = cleaner.vectorized_clean(test_texts)
        clean_time = time.time() - start_time
        
        print(f"æ¸…ç† {len(test_texts)} ä¸ªæ–‡æœ¬è€—æ—¶: {clean_time:.2f}ç§’")
        print(f"å¹³å‡æ¯ä¸ªæ–‡æœ¬: {clean_time/len(test_texts)*1000:.2f}æ¯«ç§’")
        
        # æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½
        print("\næµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½...")
        loader = DataLoader(data_config)
        
        start_time = time.time()
        industry_df = loader.load_industry_data()
        load_time = time.time() - start_time
        print(f"åŠ è½½ {len(industry_df)} æ¡è¡Œä¸šæ•°æ®è€—æ—¶: {load_time:.2f}ç§’")
        
        start_time = time.time()
        chunks = loader.load_enterprise_chunks()
        load_time = time.time() - start_time
        total_enterprises = sum(len(chunk[1]) for chunk in chunks)
        print(f"åŠ è½½ {total_enterprises} æ¡ä¼ä¸šæ•°æ®è€—æ—¶: {load_time:.2f}ç§’")
        
        print("\næ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        
    finally:
        shutil.rmtree(temp_dir)

def main():
    """ä¸»å‡½æ•°"""
    print("ä¼ä¸šç»è¥èŒƒå›´ä¸è¡Œä¸šåˆ†ç±»åŒ¹é…ç³»ç»Ÿ - å®Œæ•´åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥ä¾èµ–
    print("æ£€æŸ¥ä¾èµ–åŒ…...")
    try:
        import torch
        import faiss
        import sentence_transformers
        print("âœ“ æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    except ImportError as e:
        print(f"âœ— ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·è¿è¡Œ: pip install -r requirements_bge_faiss.txt")
        return 1
    
    # è¿è¡Œå•å…ƒæµ‹è¯•
    print("\nè¿è¡Œå•å…ƒæµ‹è¯•...")
    suite = create_test_suite()
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    if result.wasSuccessful():
        run_performance_test()
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"è¿è¡Œæµ‹è¯•: {result.testsRun}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    print(f"è·³è¿‡: {len(result.skipped)}")
    
    if result.wasSuccessful():
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é¡¹ç›®åŠŸèƒ½æ­£å¸¸ã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. å‡†å¤‡çœŸå®æ•°æ®æ–‡ä»¶")
        print("2. è¿è¡Œ python main.py å¼€å§‹åŒ¹é…")
        print("3. æŸ¥çœ‹ç”Ÿæˆçš„ç»“æœæ–‡ä»¶")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1

if __name__ == '__main__':
    exit(main())